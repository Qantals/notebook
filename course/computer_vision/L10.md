# Recurrent Neural Networks(循环神经网络)

## process sequences
- one to one: vanilla network
- one to many(decoder): image captioning
- many to one(encoder): sentiment classification
- many to many: machine translation, video classification(on frame level)
- non-sequence data: take a series of "glimpse"

## structure
- update internel state when new x inputs, feed this state as input next time. Usually want to output at some steps: FC layer.
- recurrence formula: $h_t=f_W(h_{t-1},x_t)$, $h$ is state, $x$ is a sequence of vectors
- every step use same $f$ and $W$, intermediate states are "hidden"
- vanilla recurrent neural network: $h_t=tanh(W_{hh}h_{t-1}+W_{xh}x_t)$, $y_t=W_{hy}h_t$, $W_{ab}$ means from a to b.
> $x,h$ are stack together as input, so weight can be stack as well, increasing dimension.

## forward/backward
- backpropagation: reuse the same block, need sum up the gradients. $L=\sum_i L_i$
- sampling: (take Softmax of output score to obtain probability distribution) from previous output and put it to next input.
    - use Softmax to sample may get diversity of output which is nice
- example: character-level language model: send one character at a time, transform a word to vectors where each slot =1 means appearance of a character, =0 means not appear.
- truncated backpropagation through time
    - normal forward/back propagation cost much time till the end of sequence.
    - forward/back through chunks of sequence: sub-sequence.
    - carry hidden states next batch, full forward but only this incremental states using backward
- search for interpretable cells: at different states the value of picked hidden vector(state).
    - find quotes, count numbers of words, find if statement

## application
- image captioning: CNN+RNN
    - CNN output features(cut final FC and Softmax), add image information in each step $v:h=tanh(W_{xh}*x+W_{hh}*h+W_{ih}*v)$
    - start with some initial words
    - sample \<END\> token means finish(already set in training).
- image captioning with attention: CNN returns vectors at each region of image, then RNN returns attention distribution region at first, get attention region vector(from feature map of any chosen region) as next input so RNN outputs some vocabularies and next attention region.
    - functions are differentiable, training is feasible
- video questioning: CNN+RNN-attention
- Multilayer RNNs: last layer RNN output as next layer RNN input

## gradient
- vanilla RNN gradient flow
    - gradient of $h_0$ involves powers of $W_{hh}^T$
    - largest singular value >1: exploding gradients, using gradient clipping: scale gradient down if L2-norm is too big
    - largest singular value <1: vanishing gradients, using more completed RNN structure(LSTM, against vanilla one)
- long short term memory(LSTM)
    - four functions $\begin{pmatrix} i \\ f \\ o \\ g \end{pmatrix}=\begin{pmatrix} \sigma \\ \sigma \\ \sigma \\ tanh \end{pmatrix}W\begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix}$
    - has two hidden states: $c_t=f\odot c_{t-1}+i\odot g,h_t=o\odot tanh(c_t)$
        - They are made to compute result $y_t$
    - functions are independent scalar values(indicate binary 1,0)(due to $tanh$, sigmoid)
        - f:forget gate, where to erase cell
        - i:input gate, whether to write
        - g:gate gate, how much to write
        - o: output gate, how much to reveal
    - W: composed of different weights of four gates($2h\times 4h$)
    - gradient flow $c_0$ easy: from $c_t$ to $c_{t-1}$ only need elementwise multiplication by $f$, no matrix multiplication $W$ -> highway(like ResNet)
        1. elementwise multiplication is easier than matrix one
        2. forget gate is different in each cell
        3. forget gate value is between 1/0
        4. no so much $tanh$ along each cell
        5. possible for gradient vanishing, so TRICK: initialize $f_0$ manually
    - gradient on $W$: is a sequence, but local gradient $c$ is clean.
    - similar inspiration: Highway Networks

## other RNN variants:
- GRU(gate recurrent unit): like LSTM multiplicative element wise gates to overcome gradient vanishing problem

