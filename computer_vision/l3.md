# Loss Functions and Optimization

## preprocessing-normalization
- subtracting mean of training data from training, test, validation data by dimension-wise.

## linear classifier
- which W is better?(optimization)

## loss function/cost function/objective
- quality of classifier
- $x_i$ is image, $y_i$ is label(integer), $L=\frac{1}{N}\sum_i L_i$
- **multiclass SVM loss**: hinge loss/max margin loss(threshold at zero $max(0,-)$ function) $L_i=\sum_{j\neq s_i}\max(0,s_j-s_{y_i}+\Delta)$ where $s_j=f(x_j,W)$ is safety margin, $i$ means target category, $j$ means other categories, $y_i$ means ground truth label
- constant margin $\Delta=+1$ doesn't care, because the scale of $W$ directly determines the score
- not easy to interpret the score
- min=0, max=$\infty$
> initial W makes s similar to each other, so $L_i=C-1$ C=num(class)  
> if admit $j=i$ then all $L_i$ increase by one, thus no $L_i=0$  
> if convert mean of $L$ to sum, the result won't change(just scaling)  
> if square the $max()$ function, the result will be different.(squared hinge loss SVM/L2-SVM)

## Regularization
- no **unique** W for $L=0$ (e.g. 2W, 4W, ...)
- overfit: care about test performance, rather than training data. So we prefer simpler(intuitive) $W$
- purpose: soft constrain to access more complex model
- usually not regular the bias, and only for weights. The result is negligible.
- regularization penalty: $R(W)$, the loss function is composed of *Data Loss* + *Regularization Loss*(hyperparameters: $\Delta$ & $\lambda$)
- Regularization Loss =$\lambda R(W)$, $\lambda$ is hyperparameter(usually set in cross validation)
- common use regression
    - L2=$\sum W_{k,l}^2$ (get W2=\[.25 .25 .25 .25\]care about every $x$ element for not too large -> more generalization, more diffuse)
    - L1=$\sum |W_{k,l}|$ (get W1=\[1,0,0,0\] encourages sparsity , opposite to L1)
    - elastic net(L1+L2)=$\sum \beta W_{k,l}^2+|W_{k,l}|$
    - max norm, dropout, fancier(batch normalization, stochastic depth)

## softmax classfier(multinomial logical regression)
- cross-entropy(熵): $H(p,q)=-\sum_x p(x)\log{q(x)}$, where $p$ is ture distribution, $q$ is estimated distribution. cross-entropy loss attempts to minimize the cross-entropy
- target: get probability distribution $P$(softmax function) to match real probability distribution(0-1 distribution), more intuitively than SVM score
- normalization: want to maximize log(monotonic function)(maximum likelihood estimation-MLE) / minimize negative log of correct class(low probability means large loss) $$L_i=-\log{P(Y=k|X=x_i)}=-\log{\frac{e^{s_{y_i}}}{\sum_j e^{s_j}}}$$
> min=0, max=$\infty$  
> initial W makes s close to 0, so $L_i=-\log{\frac{1}{C}}=\log{C}$ C=num(class)
- **numerical stability**: normalization trick used when exponentials are quite huge. Multiply C both on top and down of fraction, common use $\log{C}=-\max_j f_j$ by subtracting $\max f$ to set the $f_{max}=0$

## difference of these two loss functions
- multiclass SVM: care about the bar, till correct class score higher than incorrect class to absolute scale.(local objective: see as feature/bug, focus only on difficult problems: similar things, rather than totally different classes)
- softmax: always push the gap between the correct and incorrect class.
- compare: softmax is better shown as probability, but in practice the difference is negligible.

## Optimization
- visualize the loss function: slice high dimension to ray/plane, you can see bowl-shaped appearance for SVM loss(depend on the form of SVM loss --> max(0,-) is linear function)(convex function, corresponding to convex optimization)
- SVM loss: non-differentiable, but has subgradient(like $f(x)=|x|$ at $x=0$ but has a set \[-1,1\])
- Core idea: iterative refinement, find best is hard, but find better one is OK.
1. first very bad idea: random search
2. improvement: random local search: get random pertubations for direction and keep the useful one.
3. local geometry: follow the slope(multiple dimensions: gradient, calculated by partial derivation) drops to 0, so follow the *negative gradient* to steepest direction
    - numerical gradient: partial derivation once a time, iterating all $W$ (approximate, slow, easy)
        > can be used in gradient check(debug)  
        > use Frobenius norm can check the difference of two matrix $W$
        - normal formula: usually increasement $h=1e-5$ is OK
        - centered difference formula
    - in practice: use calculus to compute analytic gradient(exact, fast, error-prone)
        - optimization in primal: objects are not technically differentiable
        - 1 function: indicator function
- gradient descent: need step size/learning rate(hyperparameter, **more important than model size / regularization**)
    - fancier optimization manners: momentum, Adam optimizer
    - Mini-batch gradient descent(MGD, or BGD-batch): because N is large and computing sum is slow, in practice approximate sum using mimibatch examples of training set(commonly for 32/64/128), similar to Monte Carlo estimation
        - stochastic gradient descent(SGD)/on-line gradient descent: sample only one example to compute the gradient. Not frequently used in practice, because vectorized acceleration is useful.
        - reason: examples in training data are correlated(更新参数更快，引入额外噪声，有利于更鲁棒地收敛，避免局部最优)
- other manners: L-BFGS

## Aside: Image Features
- feed linear classfiers with extracted feathers rather than pixelwise data
- purpose: easy to draw decision boundary of linear one
- example manner #1: color histogram to statistics times appear in color blocks
- example manner #2: Histogram of Oriented Gradients(HoG-梯度方向直方图): capture edges in divided bins and quantify
- example manner #3: bag of words:(inspired from NLP)
    1. extract random patches from images, then cluster these to form "codebook" using K-means.
    2. encode images and count visual "words"
- feature extractor is fixed while training. CNN: learn features directly from data

## An illustration of picture of 1-D data loss
see x-axis means $W_0$, make $x_0,x_1,x_2 >0$, thus three lines means the first term of $L_1,L_2,L_3$. The reverse line means term of $L_0$, because coefficient of $W_0$ is negative. Different slope corresponding to differert $x$.