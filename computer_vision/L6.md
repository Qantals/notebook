# training neural network I

## activation functions
- sigmoid $\sigma(x)=1/(1+e^{-x})$
    - squashes numbers to $[0,1]$ (firing rate), like saturating firing rate of a neuron
    - problem:
        1. **more severe** saturate kill gradients(close to 0);
        2. not zero centered: input to a deep neuron is always positive, gradient of $f=W^Tx+b$ on $W$ is always positive, so gradient of $L$ on $W$ is all positive or negative(depend on $\frac{dL}{df}$), lead to low efficiency(zig zag path) in optimization.
        3. exp() is expensive
- tanh
    - squashes to $[-1,1]$
    - zero centered
    - stature kill gradients
- ReLU- rectified linear unit线性整流函数，修正线性单元 $max(0,x)$
    - does not saturate in positive region
    - easy to compute
    - converges faster than tanh/sigmoid about 6X
    - problem:
        1. not zero centered
        2. saturate in negative region
        3. dead ReLU: never activate, never update. Depend on initial weights and learning rate. In practice give positive bias(0.01, and positive region is not saturate) initially may be helpful.
- leaky ReLU $max(0.01x,x)$
    - does not saturate
    - easy to compute
    - converges faster than tanh/sigmoid about 6X
    - don't "die"
- parametric Rectifier(PReLU) $max(\alpha x,x)$
- ELU(exponential linear unit), between ReLU and leaky ReLU $x(x\ge 0);\alpha(e^x-1)(x\lt 0)$
    - all benefits of ReLU
    - output average close to 0
    - negative saturation regime adds robustness to noise
    - exp() is expensive
- maxout $max(w_1^Tx+b_1,x_2^Tx+b_2)$
    - nonlinearity
    - generalizes ReLU and leaky ReLU
    - linear regime(no saturate, no death)
    - problem: doubles parameters/neurons
- suggestion: 
    1. first ReLU(careful with learning rate)
    2. leaky ReLU/maxout/ELU
    3. last try out tanh but don't expect much
    4. Don't use sigmoid!

## data preprocessing
- used in training and testing
- don't change the structure of data, like gaussian distribution
- zero-centered: x-=mean(x), easy for gradient desent
    - in practice, subtract mean image(32\*32\*3) or per-channel mean(32\*32\*1)
    - take mean of whole training/test set
- normalize: x/=std(x), less sensitive to small changes in weights(data cloud close to center), easy to optimize. Not generally used in images
- PCA, Whitening(complex, not generally used in images)

## weight initialization
- initialize equally: all neurons behave the same
- first idea: small random numbers(gaussian distribution)
    - works okay for small networks but not deeper ones, because activation function like tanh(non-linearity) lead to small input to deep layers, standard deviation sharps down. And gradients are very small due to small input $x$ for each gate, chain rule multiply these lead to small.
    - if random numbers is big(0.01->1), all neurons saturated, +1/-1. Gradients are 0.
- Xavier initialization: keeps variance(方差) of input is the same as output(big input matches to small weights, vice versa).
    - ReLU: kill half for variance.

## batch normalization(批标准化)
- make unit gaussian activations in each layer $\hat{x}^{(k)}=\frac{x^{(k)}-E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}$, differentiable.
- mean is for each dimension across N batches
- Usually inserted after FC layers, before nonlinearity
- flexibility: scale and squash/shift: $y^{(k)}=\gamma^{(k)}\hat{x}^{(k)}+\beta^{(k)}(\gamma =\sqrt{Var[x]},\beta =E[x])$ , recover the identity(not tight) mapping
- normalize with mini-batch mean and variance, but scale and shift with emperial(learned $\gamma ,\beta$) mean and variance: like relularization.
- learning: computed from training by running averages, used in training and testing.

## babysitting the learning process
- preprocessing, choose architecture
- double (sanity) check the loss is reasonable: disable regularization, initialize specially and check expectation of special loss function.
- start training with small batch(need overfit), decrease regularization see in each epoch(forward and back) if loss decreases to 0.
- Finish sanity check, start training. Hyperparameters: learning rate(adjust first).
    - loss not decrease: learning rate too low
    - loss holds but training accuracy increases to 20% for softmax: score is diffuse but every epoch adjust the weights.
    - loss NaN(exploding): learning rate too high

## hyperparameter optimization
- cross validation in stages: coarse(few epochs and tough ideas) -> fine(longer running time, but in practice not over coarse 3 times)
- step: log space(power, rather than uniform sampling)
- make sure exploration space is sufficient(cross-validation should not close to the space edge)
- random search is better than grid search: not lose good points
- including
    1. network architecture
    2. learning rate, decay schedule, update type
    3. regularization (L2/dropout strength?)
- good learning rate: no explosion, no bottleneck
- loss holds at first and drop suddenly: bad initialization
- monitor and visualize accuracy: big gap(between training and validation set)=overfitting, check regularization; no gap= need to increase model capacity.
- check radio of weight updates/ weight magnitudes $\approx$ 0.001
