## compression of images
- image: $A_{512\times 512}\to x\in \mathbb{R^{512\times 512}}$ one vector represents whole image
- standard basis $(0,0,0,1,0,0)^T$
- better basis
    - low frequency signal $(1,1,1,1,1,1)^T$, pure black image
    - high frequency signal $(1,-1,1,-1,1,-1)^T$, chessboard image
    - $(1,1,1,-1,-1,-1)^T$, half light half dark image
- Fourier basis: divide into $8\times 8$ blocks in the image
    - columns of Fourier matrix $(1,1,\dots)^T,(1,w,w^2,w^3,\dots)^T,\dots$
    - each block: 64 cofficients, 64 basis for 64 pixels $\to x\in \mathbb{R^{64}}$.
- lossy compression
    - set threshold to remove coefficients with many zeros (normal case throw high frequency because things changes smoothly)
    - $\hat{x}=\sum _{i<64}\hat{c_i}v_i$
- wavelets 小波 for $\mathbb{R^8}$
    - $w_1=(1,1,1,1,1,1,1,1)^T,w_2=(1,1,1,1,-1,-1,-1,-1)^T$
    - $(1,1,-1,-1,0,0,0,0)^T,(0,0,0,0,1,1,-1,-1)^T$
    - $(1,-1,0,0,0,0,0,0)^T,(0,0,1,-1,0,0,0,0)^T$
    - $(0,0,0,0,1,-1,0,0)^T,(0,0,0,0,0,0,1,-1)^T$
    - $p_{8\times 1}=c_1w_1+\dots +c_8w_8=Wc\to c=W^{-1}p=W^Tp$ orthogonal
- good basis
    - computes fast (FFT,FWT)
    - good compression, few is enough
- eigenvectors as basis is good $\to A=\Lambda$

## change of basis
- $T:V\in \mathbb{R^n}\to V\in \mathbb{R^n}$
- change of basis: $\alpha=(\alpha _1,\dots ,\alpha _n),\beta =(\beta _1,\dots ,\beta _n)\in V\in \mathbb{R^n}$ are different basis, $\beta =\alpha S(\beta _j=\sum _iS_{ij}\alpha _i)\to S_{n\times n}$ is transition matrix
- change of coordinates: $\forall v\in V\to v=\alpha x=\beta y=\alpha Sy$ ($x_{n\times 1},y_{n\times 1}$ are coordinates) $\to x=Sy$
- similarity $A\sim B\Leftrightarrow$ same eigenvalues $\to$ same linear transformation under different basis
    > proof: $A$ is transformation matrix corresponding $T$ with basis $\alpha$, $B$ is with $\beta$  
    > $\therefore$ see from basis $\alpha: u=Ax=SBy=SBS^{-1}(Sy)=(SBS^{-1})x\to A=SBS^{-1}$ is similar
    - $Ax=SBy\Leftarrow R=S\to u=Ax=Sv=S(By)$ see below
- expansion to $m\ne n$
    - $T:V\in \mathbb{R^n}\to W\in \mathbb{R^m}$
    - $V\in \mathbb{R^n}:\alpha \to \beta:\beta =\alpha S_{n\times n},x=Sy$
    - $W\in \mathbb{R^m}:\gamma \to \delta:\gamma =\delta R_{m\times m},u=Rv$
    - $w=T(v)=\gamma u=\delta v$
    - $u=Ax,v=By=BS^{-1}Sy=BS^{-1}x=R^{-1}u=R^{-1}Ax\to R^{-1}A=BS^{-1},B=R^{-1}AS$
