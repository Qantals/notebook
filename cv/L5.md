# convolutional neural networks

> history
> - perceptron algorithm, $f(x)=1$ (if $wx+b>0$);0(otherwise). update rule is informal.
> - 2006, Hinton use restricted Boltzmann machines, initialize carefully then tuning parameters.
> - 2012, first strong results: sound recognition, AlexNet
> - biological experiments: different cells response to different features(corners, blobs)
> - 1998, Yann LeCun use CNN LeNet-5 to recognize postcode with backpropagation
> - now, parallel processing ...

- fully connected layer: compress to one number(matrix multiplication, linear classifier), the filter scan the whole image, different from convolution one.
- convolution layer:
    - preserve spatial structure(32\*32\*3), use filter(5\*5\*3 $w$ with a bias $b$, depth should be same as image) do dot product($w^Tx+b$ get one value) and slide over(28\*28\*3).
    - filters:
        - Many filters: many activation maps.
        - receptive field(感受野): size of filter
        - different filters capture same location in image but different features.
    - With activation coming with every convolution composed a sequence(may have different amount of filters).
        - higher levels seek for more complex features. Then use linear classifier to spare
- slide output size: $(N-F)/stride+1$.
    - want to obtain full size: in practice pad 0 to broder, increase $N$ twice.
    - common settings: K=powers of 2(32,64,128,256)
        - F=3, S=1, P=1
        - F=5, S=1, P=2
        - F=5, S=2, P=?(whatever fits)
        - F=1, S=1, P=0
- pooling layer: downsampling(but depth keeps), make representations smaller and manageable, operate independently on each activation map.
    - max pooling: no overlap, better than average(intuitive illustration: find significant feature)
    - common settings:
        - F=2, S=2
        - F=3, S=2(OK)
- trend: smaller filters and deeper architectures; getting rid of POOL/FC layers, just CONV;
- typical architecture: 
    - <p>[(CONV-RELU)*N-POOL]*M-(FC-RELU)*K,SOFTMAX</p>
    - N up to 5, M is large, 0<=K<=2
