# image classification
core in CV: image classification
- semantic segmentation

challenges: pixels change a bit(robustness), illumination, deformation, occlusion, background clutter, intraclass variation
wish: capture commen features of the same class, get rid of irrelevant characteristics(e.g. background)

one way of CV: edges -> corners(bad result)

API: image -> class_label

dataset: CIFAR10(10 class,50k,10k training & testing, 32*32*3 image)

classifier: data-driven apporach:
- train and predict

## first classifier: nearest neighbour
- memorize all data and labels
- predict according to all training data
- but need pridiction fast and training slow/OK

## distance metrics
- L1 distance(Manhattan distance)(corellates to coordinate system, equals to input values related to tasks)
- L2 distance(Euclidean distance)(don't know meaning of input)
    can leave out in practice, because it's **monotonic function**(scales absolute value, preserve ordering)
    - *unforgive*
- link to p-norm

## K-nearest neighbours
- instead of copying nearests' labels, take majority vote from K cloest points. *smooth effect*
- issues: distance metrics not informative(for high-dimension pixel-based distance)(rely on background, rather than semantic content), test slow, curse of dimensionality(high density)

## Approximate Nearest Neighbour library(e.g. [FLANN](https://github.com/flann-lib/flann))
faster but lose accuracy

## hyperparameters: choices about algorithms(K, distance metrics)
> problem-dependent  
> test data should not be touched, otherwise overfit happens
- idea #1: hyperparameters work best on training data(bad: well only on training)
- idea #2: spilt dataset to train and test, choose work best on test data(bad: overfitting)
- idea #3: spilt to train, validation and test, ~ work best on validation
> spilt should be independent and justify
- idea #4: cross validation, spilt to N folds and test and take folds as validation in turn(when data scale is small)

## cross-validation
- more folds, more smooth(less noise)
- compute expensively
- more hyperparameters, more validation scale
- in practice, not used for last version's training, see as setting hyperparameters
- in practice: 50%~90% for training and rest for validation, 3/5/10 folds

## linear classification
- f(x,W) -> get score represents probability -> **score function**
    - f=Wx+b f:10(K)x1, W:10x3072 x:3072(D)x1(stretch out), b:10x1(bias term, independent of training data, distinguish special categories) $f:R^D\mapsto R^K$
    - $Wx_i$ got seperate K classifiers(for each rows in $W$) runs in parallel
    - meaning: $W$ rotates the lines, $b$ controls the bias(if $x_i=0, b=0$ then $f=0$, every line cross the origin)
    - $Wx$ is template matching in fact by inner dot/dot product, and leak in matching cars of different color
    - bias trick: append bias matrix to right side of $W$, then append one dimension of value 1 to $x$
- feature: like/dislike the color/position of every pixel
- improvement: when testing, only need to forward through weights and bias vectors(fast)
- visualize weight: average every training data,learn a simple template category(bad)
- preprossing: normalization by subtracting mean of training images from every pixel(\[0,255\] -> \[-127,127\]); or scale to \[-1,1\]
- principle: from high dimension perspective, images can be regarded as a point in space, so draw lines to classify(lines mean socre 0, the side means increase/decrease)
    - restrictions: hard to draw a line to distinguish different categories