# visualing and understanding ConvNet

http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture12.pdf

## first layer: visualize filters
- see filters as images with 3 RGB channels, scale to 0~255
    > reason: input normal test vector and get this result.
- look for: oriented edges(bars of light or dark) in various angles and positions, with opposite color.
    > nearly all CNN first layer looks like this.  
    > Human visual system tend to search edges to judge.

## second layer
- view as images is not interpretable
    - too many channels to visualize
    - input is first layer but not origin image, so features may not explicit.

## last layer: FC-4096
- run with some data and compose a grid(with the same position of origin image) with these 4096-D feature vectors
- do nearest neighbour on gird of 4096-D and of origin image (similar to linear classifier classify pictures with similar pixels to same class)
- result: classify by semantic meanings rather pixels(left side and right side)
- dimensionality reduction: 4096D->2D with PCA/ t-SNE(complex, more powerful, non-linear)
    - like using in EMNIST dataset, may return back origin representation

## intermediate activations
- feature map $128\times 13\times 13$ visualize as $128\ 13\times 13$ grayscale images
- some cell turn to be specially light means it captured the feature
- maximally activating patches
    - pick a layer and a channel among them(128\*13\*13 -> 17/128)
    - run many images, record values of chosen channel, visualize image patches of corresponding maximal values
    - one neuron has limited receptional field, different layers capture different size of sturcture feature

## features
- occlusion experiments
    - mask different positions of image and run the network
    - get a hot picture of possibility
- saliency maps
    - take gradient of class score w.r.t. image pixels.
    - take absolute value and max over RGB channels(synthesis images)
    - can be used as semantic segmentation, but less helpful
- guided backprop
    - capture intermediate features using like saliency maps
    - compute gradient of intermediate neuron value w.r.t. image pixels
    - perform better if backprop positive gradient through ReLU
- gradient ascent
    - fix weights, compute gradient of class score w.r.t. input image
    - see what influence classification without specific image
    - also need regularization to prevent network specific(looks nartual)
    - initialize with zero image(or random noise), regularization using L2 norm.
    - better visualization
        1. optimize periodically:
            - Gaussian blur image
            - clip pixels with values/gradients to 0
        2. take multimodality: take something close to the class(but not exactly themselves) into account
        3. optimize FC6 instead of input pixels representation
- fooling images/ adversarial examples
    - give arbitrary image, pick arbitrary class
    - repeat modifying the image to maximize that class score until success
    - result: just add some noise. Difference is quite small

## application
- DeepDream: amplify existing features(*all features in this network*)
    - select an image and a layer
    - forward: compute gradient of chosen layer and set to its activation (amplify)
        > equivalent to maximize L2 norm of that feature(?)
    - backward: compute gradient on image and update image
    - code
        - jitter image some pixels to regularize for smoother
        - L1 normalize gradients
        - clip pixel values, project gradient to 0~255 valid image space
- feature inversion(*dependent choice of feature*)
    - give a CNN feature vector of an image
    - find new image match the vector and looks natural(regularization: total variation regularizer)
    - restructing result: high level layers drop low level details, preserve semantic information that holds variation
- texture synthesis(*all features*)
    - generate bigger image of sample patch texture image
    - nearest neighbour: generate one at a time in scanline order
    - Gram Matrix
        - each layer gives $C\times H\times W$ tensor, see as $H\times W$ grid of C-dimensional vectors
        - take two column vectors, outer product has $C\times C$ matrix **measuring co-occurrence** (共现矩阵)
        - average over all $H*W$ paris vectors -> $C\times C$ Gram matrix(no spatial information now, nice for texture)
        - more efficient to compute than covariance matrix
        - synthesis: use Gram matrix instead of neurons through gradient ascent process(compute gradient of L2 loss between origin and new image)
            - loss: weighted sum of Gram matrices on each layer
- style transfer
    - combine texture synthesis(style matrix) and feature inversion(context image)
    - have more control than DeepDream
    - tune hyperparams: weight of two loss(content and style)
    - resize style image: different types of features
    - multiple style images: match multiple Gram matrices, like Deep
    - problem: quite slow. solution: train another feedforward network for fixed style, stylize images using single forward pass(?).
        - some paper can blend sytles

